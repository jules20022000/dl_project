{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Audio classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from model.classifier import Classifier\n",
    "from dataset.AudioDataset import AudioDataset\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification, HubertForSequenceClassification\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'\n",
    "RECORDINGS_DIR = DATA_DIR + 'recordings/'\n",
    "RESULTS_DIR = './results/'\n",
    "WAVEFORMS_DIR = DATA_DIR + 'waveforms/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_df = pd.read_csv(DATA_DIR + 'overview-of-recordings-label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal in this section is to convert audio files into waveforms and subsequently train a classifier to predict the label for each audio sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Create waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the waveforms for the train, validate and test sets if they don't exist\n",
    "\n",
    "if not os.path.exists(WAVEFORMS_DIR):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(WAVEFORMS_DIR)\n",
    "    for split in ['train', 'validate', 'test']:\n",
    "        os.makedirs(WAVEFORMS_DIR + split)\n",
    "\n",
    "    for idx, row in tqdm(record_df.iterrows(), total=len(record_df)):\n",
    "        # Save the waveforms in a folder structure that is similar to the original data\n",
    "        save_path = os.path.join(WAVEFORMS_DIR, row.split, row.file_name.replace(\".wav\", \".pt\"))\n",
    "        audio = os.path.join(RECORDINGS_DIR, row.split, row.file_name)\n",
    "        # Load the audio file\n",
    "        waveform, sr = librosa.load(audio, sr=16000)\n",
    "        \n",
    "        audio_waveform_with_label = torch.cat((torch.tensor(waveform), torch.tensor([row.label])))\n",
    "\n",
    "        # Save the audio waveform\n",
    "        torch.save(audio_waveform_with_label, save_path)\n",
    "\n",
    "else:\n",
    "    print(\"Waveforms already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'data/recordings/test/1249120_44142156_61923550.wav'\n",
    "waveform, sr = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.linspace(0, len(waveform) / sr, num=len(waveform)), waveform)\n",
    "plt.title('Waveform of Audio File')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Load waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 295730\n",
    "\n",
    "train_waveforms, train_labels = load_waveforms(os.path.join(WAVEFORMS_DIR, \"train\"), max_seq_length)\n",
    "valid_waveforms, valid_labels = load_waveforms(os.path.join(WAVEFORMS_DIR, \"validate\"), max_seq_length)\n",
    "test_waveforms, test_labels = load_waveforms(os.path.join(WAVEFORMS_DIR, \"test\"), max_seq_length)\n",
    "print(\"Train embeddings shape: \", train_waveforms.shape, \"Train labels shape: \", train_labels.shape)\n",
    "print(\"Valid embeddings shape: \", valid_waveforms.shape, \"Valid labels shape: \", valid_labels.shape)\n",
    "print(\"Test embeddings shape: \", test_waveforms.shape, \"Test labels shape: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Initialize classifier and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 1000\n",
    "lr = 1e-4\n",
    "\n",
    "# Create the classifier\n",
    "classifier = Classifier(train_waveforms.shape[1], len(record_df.prompt.unique())).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses, valid_accs = train_classifier(classifier, criterion, optimizer, NUM_EPOCH, train_waveforms, train_labels, valid_waveforms, valid_labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Test classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, cm = test_classifier(classifier, test_waveforms, test_labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(classifier.state_dict(), RESULTS_DIR + \"waveforms.pt\")\n",
    "\n",
    "# Save the losses and accuracies as numpy arrays\n",
    "if not os.path.exists(RESULTS_DIR + \"waveforms\"):\n",
    "    os.makedirs(RESULTS_DIR + \"waveforms\")\n",
    "\n",
    "np.save(RESULTS_DIR + \"waveforms/tr_losses.npy\", np.array(train_losses))\n",
    "np.save(RESULTS_DIR + \"waveforms/val_accs.npy\", np.array(valid_accs))\n",
    "np.save(RESULTS_DIR + \"waveforms/val_losses.npy\", np.array(valid_losses))\n",
    "np.save(RESULTS_DIR + \"waveforms/test_acc.npy\", np.array(test_acc))\n",
    "np.save(RESULTS_DIR + \"waveforms/cm.npy\", np.array(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We also attempted to utilize waveform images and train a CNN for audio file classification; however, unfortunately as before, it did not yield successful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wav2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The objective in this section is to leverage pre-trained Wav2Vec transformers and attempt to predict the correct labels from audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-ks\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-ks\", num_labels=len(record_df.prompt.unique()), ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 295730\n",
    "batch_size = 5\n",
    "\n",
    "train_df = record_df[record_df.split == 'train']\n",
    "valid_df = record_df[record_df.split == 'validate']\n",
    "test_df = record_df[record_df.split == 'test']\n",
    "\n",
    "train_dataset = AudioDataset([os.path.join(RECORDINGS_DIR, \"train\", f) for f in train_df[\"file_name\"]], train_df.label.values, feature_extractor, max_seq_length, device)\n",
    "valid_dataset = AudioDataset([os.path.join(RECORDINGS_DIR, \"validate\", f) for f in valid_df[\"file_name\"]], valid_df.label.values, feature_extractor, max_seq_length, device)\n",
    "test_dataset = AudioDataset([os.path.join(RECORDINGS_DIR, \"test\", f) for f in test_df[\"file_name\"]], test_df.label.values, feature_extractor, max_seq_length, device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Initialize learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 15\n",
    "lr = 1e-5\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses, valid_accs = train_transformer(model, optimizer, NUM_EPOCH, train_loader, valid_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, cm = test_transformer(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), RESULTS_DIR + \"wav2vec.pt\")\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR + \"wav2vec\"):\n",
    "    os.makedirs(RESULTS_DIR + \"wav2vec\")\n",
    "\n",
    "# Save the losses and accuracies as numpy arrays\n",
    "np.save(RESULTS_DIR + \"wav2vec/tr_losses.npy\", np.array(train_losses))\n",
    "np.save(RESULTS_DIR + \"wav2vec/val_accs.npy\", np.array(valid_accs))\n",
    "np.save(RESULTS_DIR + \"wav2vec/val_losses.npy\", np.array(valid_losses))\n",
    "np.save(RESULTS_DIR + \"wav2vec/test_acc.npy\", np.array(test_acc))\n",
    "np.save(RESULTS_DIR + \"wav2vec/cm.npy\", np.array(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The objective in this section is to leverage pre-trained HuBERT transformers and attempt to predict the correct labels from audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"superb/hubert-base-superb-ks\")\n",
    "model = HubertForSequenceClassification.from_pretrained(\"superb/hubert-base-superb-ks\", num_labels=len(record_df.prompt.unique()), ignore_mismatched_sizes=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 295730\n",
    "batch_size = 5\n",
    "\n",
    "train_df = record_df[record_df.split == 'train']\n",
    "valid_df = record_df[record_df.split == 'validate']\n",
    "test_df = record_df[record_df.split == 'test']\n",
    "\n",
    "train_dataset = AudioDataset([os.path.join(RECORDINGS_DIR, \"train\", f) for f in train_df.file_name], train_df.label.values, feature_extractor, max_seq_length, device)\n",
    "valid_dataset = AudioDataset([os.path.join(RECORDINGS_DIR, \"validate\", f) for f in valid_df.file_name], valid_df.label.values, feature_extractor, max_seq_length, device)\n",
    "test_dataset = AudioDataset([os.path.join(RECORDINGS_DIR, \"test\", f) for f in test_df.file_name], test_df.label.values, feature_extractor, max_seq_length, device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Initialize learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 15\n",
    "lr = 1e-5\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses, valid_accs = train_transformer(model, optimizer, NUM_EPOCH, train_loader, valid_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, cm = test_transformer(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), RESULTS_DIR + \"hubert.pt\")\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR + \"hubert\"):\n",
    "    os.makedirs(RESULTS_DIR + \"hubert\")\n",
    "\n",
    "# Save the losses and accuracies as numpy arrays\n",
    "np.save(RESULTS_DIR + \"hubert/tr_losses.npy\", np.array(train_losses))\n",
    "np.save(RESULTS_DIR + \"hubert/val_accs.npy\", np.array(valid_accs))\n",
    "np.save(RESULTS_DIR + \"hubert/val_losses.npy\", np.array(valid_losses))\n",
    "np.save(RESULTS_DIR + \"hubert/test_acc.npy\", np.array(test_acc))\n",
    "np.save(RESULTS_DIR + \"hubert/cm.npy\", np.array(cm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
