{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice-Driven Disease Classification: A Deep Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Team 34 : Jules Maglione, Paul Nadal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import seaborn as sns\n",
    "from math import ceil, sqrt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoFeatureExtractor, HubertForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'\n",
    "RESULTS_DIR = './results/'\n",
    "RECORDINGS_DIR = DATA_DIR + 'recordings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can download the \"Medical Speech, Transcription, and Intent\" dataset from this [link](https://www.kaggle.com/datasets/paultimothymooney/medical-speech-transcription-and-intent/download?datasetVersionNumber=1). The dataset is approximately 6GB in size and includes thousands of audio utterances related to common medical symptoms such as \"knee pain\" or \"headache.\" In total, the dataset comprises over 8 hours of aggregated audio content. Each utterance has been created by individual human contributors, who based their recordings on specific medical symptoms. This extensive collection of audio snippets is suitable for training conversational agents in the medical field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata from the CSV file\n",
    "\n",
    "record_df = pd.read_csv(DATA_DIR + \"overview-of-recordings.csv\", sep=\",\")\n",
    "\n",
    "print(\"Overview of recordings: \")\n",
    "print(\"Number of recordings: \", len(record_df))\n",
    "print(\"Number of features: \", len(record_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to categorize the recordings\n",
    "\n",
    "train_files = set(os.listdir(os.path.join(RECORDINGS_DIR, \"train\")))\n",
    "valid_files = set(os.listdir(os.path.join(RECORDINGS_DIR, \"validate\")))\n",
    "test_files = set(os.listdir(os.path.join(RECORDINGS_DIR, \"test\")))\n",
    "\n",
    "record_df[\"split\"] = record_df.file_name.apply(lambda x: \"train\" if x in train_files else (\"validate\" if x in valid_files else \"test\"))\n",
    "\n",
    "# Check the distribution of the recordings\n",
    "\n",
    "print(\"Number of training samples: \", len(record_df[record_df.split == \"train\"]))\n",
    "print(\"Number of validation samples: \", len(record_df[record_df.split == \"validate\"]))\n",
    "print(\"Number of test samples: \", len(record_df[record_df.split == \"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each prompt to an integer\n",
    "prompt_to_id = {prompt: i for i, prompt in enumerate(record_df.prompt.unique())}\n",
    "id_to_prompt = {i: prompt for prompt, i in prompt_to_id.items()}\n",
    "record_df[\"label\"] = record_df.prompt.apply(lambda x: prompt_to_id[x])\n",
    "\n",
    "# Display the mapping\n",
    "print(\"Prompt to ID mapping: \")\n",
    "for prompt, id in prompt_to_id.items():\n",
    "    print(prompt, \":\", id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated CSV file \n",
    "record_df.to_csv(DATA_DIR + \"overview-of-recordings-label.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, validation, and test dataframes\n",
    "train_df = record_df[record_df['split'] == 'train']\n",
    "valid_df = record_df[record_df['split'] == 'validate']\n",
    "test_df = record_df[record_df['split'] == 'test']\n",
    "\n",
    "# Create a function to plot the count of each prompt\n",
    "def create_count_plot(ax, df, title, color):\n",
    "    sns.countplot(y='prompt', data=df, color=color, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_ylabel('Prompt')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 10))\n",
    "\n",
    "# Combine the count plots on the same row\n",
    "create_count_plot(axes[0], train_df, 'Training Set', 'blue')\n",
    "create_count_plot(axes[1], valid_df, 'Validation Set', 'green')\n",
    "create_count_plot(axes[2], test_df, 'Test Set', 'red')\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.suptitle('Count of Recordings by Prompt in Each Set', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all phrases from the training set\n",
    "all_phrases = ' '.join(train_df['phrase'])\n",
    "\n",
    "# Get unique prompts in the training set\n",
    "unique_prompts = train_df['prompt'].unique()\n",
    "\n",
    "# Calculate the number of rows and columns for the subplot grid\n",
    "num_prompts = len(unique_prompts)\n",
    "cols = ceil(sqrt(num_prompts))\n",
    "rows = ceil(num_prompts / cols)\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "\n",
    "# Generate and display word clouds for each unique prompt\n",
    "for ax, prompt in zip(axs.flatten(), unique_prompts):\n",
    "    prompt_phrases = ' '.join(train_df[train_df['prompt'] == prompt]['phrase'])\n",
    "    wordcloud = WordCloud(width=800, height=400).generate(prompt_phrases)\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(prompt, fontsize=15)\n",
    "\n",
    "# Remove empty subplots if there are more than needed\n",
    "for ax in axs.flatten()[num_prompts:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "fig.suptitle('Word Clouds for Each Prompt in the Training Set', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Audio classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Train models\n",
    "\n",
    "> See [part1.ipynb](./part1.ipynb) to know more about this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the code for part 1\n",
    "# run ./part1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_WAVEFORMS = RESULTS_DIR + \"waveforms/\"\n",
    "RESULTS_WAV2VEC = RESULTS_DIR + \"wav2vec/\"\n",
    "RESULTS_HUBERT = RESULTS_DIR + \"hubert/\"\n",
    "\n",
    "# Plot the performance of each model\n",
    "plot_model_performance(RESULTS_WAVEFORMS, \"Waveforms\")\n",
    "plot_model_performance(RESULTS_WAV2VEC, \"Wav2Vec\")\n",
    "plot_model_performance(RESULTS_HUBERT, \"HuBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Experiment by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record a sample audio file\n",
    "speech_to_wav(DATA_DIR + \"sample.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model and load the weights\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"superb/hubert-base-superb-ks\")\n",
    "model = HubertForSequenceClassification.from_pretrained(\"superb/hubert-base-superb-ks\", num_labels=len(record_df.prompt.unique()), ignore_mismatched_sizes=True).to(device)\n",
    "model.load_state_dict(torch.load(RESULTS_DIR + \"hubert.pt\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the prompt for the sample audio file\n",
    "y, sr = librosa.load(DATA_DIR + \"sample.wav\", sr=16000)\n",
    "inputs = feature_extractor(y, sampling_rate=sr, return_tensors=\"pt\")\n",
    "logits = model(inputs.input_values.to(device), attention_mask=inputs.attention_mask.to(device)).logits\n",
    "preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Predicted prompt: \", id_to_prompt[preds.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Train models\n",
    "\n",
    "> See [part2.ipynb](./part2.ipynb) to know more about this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the code for part 2\n",
    "# run ./part2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_EMBEDDINGS = RESULTS_DIR + \"embeddings/\"\n",
    "RESULTS_BASE_BERT = RESULTS_DIR + \"base_bert/\"\n",
    "RESULTS_LARGE_BERT = RESULTS_DIR + \"large_bert/\"\n",
    "\n",
    "\n",
    "# Plot the performance of each model\n",
    "plot_model_performance(RESULTS_EMBEDDINGS, \"Embeddings\")\n",
    "plot_model_performance(RESULTS_BASE_BERT, \"Base BERT\")\n",
    "plot_model_performance(RESULTS_LARGE_BERT, \"Large BERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Experiment by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a sample phrase\n",
    "phrase = \"My foot hurts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model and load the weights\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(record_df.prompt.unique())).to(device)\n",
    "model.load_state_dict(torch.load(RESULTS_DIR + \"base_bert.pt\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the prompt for the sample phrase\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "logits = model(inputs.input_ids.to(device), attention_mask=inputs.attention_mask.to(device)).logits\n",
    "preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Predicted prompt: \", id_to_prompt[preds.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Audio to text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Train models\n",
    "\n",
    "> See [part3.ipynb](./part3.ipynb) to know more about this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the code for part 3\n",
    "#  run ./part3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_SPEECH_TO_TEXT = RESULTS_DIR + \"speech_to_text/\"\n",
    "RESULTS_BERT_ON_ASR = RESULTS_DIR + \"bert_on_asr/\"\n",
    "\n",
    "print(f'Speech-to-Text Accuracy (word by word): {np.load(RESULTS_SPEECH_TO_TEXT + \"test_acc.npy\") * 100:.2f}%')\n",
    "\n",
    "# Plot the performance of each model\n",
    "plot_model_performance(RESULTS_BERT_ON_ASR, \"BERT-on-ASR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Experiment by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record a sample audio and convert it to text\n",
    "phrase = speech_to_text()\n",
    "print(\"Phrase: \", phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model and load the weights\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(record_df.prompt.unique())).to(device)\n",
    "model.load_state_dict(torch.load(RESULTS_DIR + \"base_bert.pt\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the prompt for the sample phrase\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "logits = model(inputs.input_ids.to(device), attention_mask=inputs.attention_mask.to(device)).logits\n",
    "preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Predicted prompt: \", id_to_prompt[preds.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
