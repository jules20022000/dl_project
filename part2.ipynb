{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from model.classifier import Classifier\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.PhraseDataset import PhraseDataset\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'\n",
    "RESULTS_DIR = './results/'\n",
    "EMBEDDINGS_DIR = DATA_DIR + 'embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_df = pd.read_csv(DATA_DIR + 'overview-of-recordings-label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The objective in this section is to leverage the BERT tokenizer and model to generate embeddings for the text phrases. Subsequently, a custom classifier will be employed to predict the label for each phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings for the train, validate and test sets if they don't exist\n",
    "\n",
    "if not os.path.exists(EMBEDDINGS_DIR):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(EMBEDDINGS_DIR)\n",
    "    for split in ['train', 'validate', 'test']:\n",
    "        os.makedirs(EMBEDDINGS_DIR + split)\n",
    "\n",
    "    for idx, row in tqdm(record_df.iterrows(), total=len(record_df)):\n",
    "        # Save the embeddings in a folder structure that is similar to the original data\n",
    "        save_path = os.path.join(EMBEDDINGS_DIR, row.split, row.file_name.replace(\".wav\", \".pt\"))\n",
    "        sentence = row.phrase\n",
    "        tokenized_text = tokenizer.tokenize(sentence)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        model.eval()\n",
    "        # Get hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens_tensor, segments_tensors)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # take the mean of the embeddings over the whole sentence\n",
    "        sentence_embedding = torch.mean(hidden_states[0], dim=0)\n",
    "        \n",
    "        # !! NOTE: we add the label at the end of the embedding (hence 768 for the embedding size and 1 for the label)\n",
    "        # Also note that for the large bert, the embedding size is 1024 instead of 768\n",
    "        sentence_embedding_with_label = torch.cat((sentence_embedding, torch.tensor([row.label])))\n",
    "        \n",
    "        # Save the embedding\n",
    "        torch.save(sentence_embedding_with_label, save_path)\n",
    "\n",
    "else:\n",
    "    print(\"Embeddings already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings, train_labels = load_embeddings(os.path.join(EMBEDDINGS_DIR, \"train\"))\n",
    "valid_embeddings, valid_labels = load_embeddings(os.path.join(EMBEDDINGS_DIR, \"validate\"))\n",
    "test_embeddings, test_labels = load_embeddings(os.path.join(EMBEDDINGS_DIR, \"test\"))\n",
    "print(\"Train embeddings shape: \", train_embeddings.shape, \"Train labels shape: \", train_labels.shape)\n",
    "print(\"Valid embeddings shape: \", valid_embeddings.shape, \"Valid labels shape: \", valid_labels.shape)\n",
    "print(\"Test embeddings shape: \", test_embeddings.shape, \"Test labels shape: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Initialize classifier and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 5000\n",
    "lr = 0.0001\n",
    "embeddings_trained = False\n",
    "\n",
    "# Create the classifier\n",
    "classifier = Classifier(train_embeddings.shape[1], len(record_df.prompt.unique())).to(device)\n",
    "if os.path.exists(RESULTS_DIR + 'embeddings.pt'):\n",
    "    embeddings_trained = True\n",
    "    classifier.load_state_dict(torch.load(RESULTS_DIR + 'embeddings.pt'))\n",
    "    print(\"Loaded classifier from disk\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embeddings_trained:\n",
    "    train_losses, valid_losses, valid_accs = train_classifier(classifier, criterion, optimizer, NUM_EPOCH, train_embeddings, train_labels, valid_embeddings, valid_labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Test classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, cm = test_classifier(classifier, test_embeddings, test_labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "if not embeddings_trained:\n",
    "    torch.save(classifier.state_dict(), RESULTS_DIR + \"embeddings.pt\")\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR + \"embeddings\"):\n",
    "    os.makedirs(RESULTS_DIR + \"embeddings\")\n",
    "\n",
    "# Save the losses and accuracies as numpy arrays\n",
    "if not embeddings_trained:\n",
    "    np.save(RESULTS_DIR + \"embeddings/tr_losses.npy\", np.array(train_losses))\n",
    "    np.save(RESULTS_DIR + \"embeddings/val_accs.npy\", np.array(valid_accs))\n",
    "    np.save(RESULTS_DIR + \"embeddings/val_losses.npy\", np.array(valid_losses))\n",
    "np.save(RESULTS_DIR + \"embeddings/test_acc.npy\", np.array(test_acc))\n",
    "np.save(RESULTS_DIR + \"embeddings/cm.npy\", np.array(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal in this section is to perform direct phrase classification using a pre-trained and well-known transformer model: BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels=len(record_df.prompt.unique())).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 37\n",
    "batch_size = 256\n",
    "\n",
    "train_df = record_df[record_df.split == 'train']\n",
    "valid_df = record_df[record_df.split == 'validate']\n",
    "test_df = record_df[record_df.split == 'test']\n",
    "\n",
    "train_dataset = PhraseDataset(list(train_df.phrase), train_df.label.values, tokenizer, max_seq_length, device)\n",
    "valid_dataset = PhraseDataset(list(valid_df.phrase), valid_df.label.values, tokenizer, max_seq_length, device)\n",
    "test_dataset = PhraseDataset(list(test_df.phrase), test_df.label.values, tokenizer, max_seq_length, device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Initialize learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 15\n",
    "lr = 1e-5\n",
    "large_bert_trained = False\n",
    "\n",
    "if os.path.exists(RESULTS_DIR + 'large_bert.pt'):\n",
    "    large_bert_trained = True\n",
    "    model.load_state_dict(torch.load(RESULTS_DIR + 'large_bert.pt'))\n",
    "    print(\"Loaded large bert from disk\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not large_bert_trained:\n",
    "    train_losses, valid_losses, valid_accs = train_transformer(model, optimizer, NUM_EPOCH, train_loader, valid_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, cm = test_transformer(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "if not large_bert_trained:\n",
    "    torch.save(model.state_dict(), RESULTS_DIR + \"large_bert.pt\")\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR + \"large_bert\"):\n",
    "    os.makedirs(RESULTS_DIR + \"large_bert\")\n",
    "\n",
    "# Save the losses and accuracies as numpy arrays\n",
    "if not large_bert_trained:\n",
    "    np.save(RESULTS_DIR + \"large_bert/tr_losses.npy\", np.array(train_losses))\n",
    "    np.save(RESULTS_DIR + \"large_bert/val_accs.npy\", np.array(valid_accs))\n",
    "    np.save(RESULTS_DIR + \"large_bert/val_losses.npy\", np.array(valid_losses))\n",
    "np.save(RESULTS_DIR + \"large_bert/test_acc.npy\", np.array(test_acc))\n",
    "np.save(RESULTS_DIR + \"large_bert/cm.npy\", np.array(cm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
